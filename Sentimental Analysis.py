# -*- coding: utf-8 -*-
"""Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P0yyXftv_WLdRR2J3ryEzOJ3hVZrv8LL
"""

# Import necessary libraries
import pandas as pd
import statsmodels.api as sm
from google.colab import files

# Step 1: Upload the file
uploaded = files.upload()

# Step 2: Load the dataset
file_name = list(uploaded.keys())[0]  # Get the uploaded file name
data = pd.ExcelFile(file_name)  # Load the Excel file
df = data.parse('AmazonFoodCategory_1 Dataset')  # Load the relevant sheet

# Step 3: Data Preparation - Select relevant columns for regression
# Example: Using Sales Amount as dependent and Discount Amount, List Price, and Sales Quantity as independent
df = df[['Sales Amount', 'Discount Amount', 'List Price', 'Sales Quantity']]

# Drop rows with missing values
df = df.dropna()

# Step 4: Define dependent and independent variables
X = df[['Discount Amount', 'List Price', 'Sales Quantity']]  # Independent variables
y = df['Sales Amount']  # Dependent variable

# Add a constant to the independent variables (for the intercept)
X = sm.add_constant(X)

# Step 5: Fit the regression model
model = sm.OLS(y, X).fit()

# Step 6: Display the regression summary
print(model.summary())

# Step 3: Data Preparation
# Check for missing values in the dataset
print("Missing values per column:\n", df.isnull().sum())

# Drop rows with missing values (optional: replace with mean/median if necessary)
df = df.dropna()

# Verify there are no infinite or NaN values
print("Checking for invalid values after cleaning:")
print(df.isnull().sum())  # Ensure no NaN
print((~df.isin([float('inf'), float('-inf')])).all())  # Ensure no infinities

# Proceed with defining binary classification target
df['Sales_Binary'] = (df['Sales Amount'] > df['Sales Amount'].median()).astype(int)

# Selecting independent variables
X = df[['Discount Amount', 'List Price', 'Sales Quantity']]  # Independent variables
y = df['Sales_Binary']  # Binary dependent variable

# Split data into train and test sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Add constant to independent variables for regression
import statsmodels.api as sm
X_train = sm.add_constant(X_train)
X_test = sm.add_constant(X_test)

# Fit the logistic regression model
log_model = sm.Logit(y_train, X_train).fit()

# Display the regression summary
print(log_model.summary())

# Import necessary libraries
import pandas as pd

# Step 1: Load the dataset
uploaded = files.upload()  # Upload file to Colab
file_name = list(uploaded.keys())[0]
data = pd.ExcelFile(file_name)

# Parse the relevant sheet
df = data.parse('AmazonFoodCategory_1 Dataset')

# Step 2: Calculate metrics for CLV
# Assume 'Sales Amount' is the revenue, 'Custkey' is the customer identifier
# Step 2a: Total Revenue per Customer
customer_revenue = df.groupby('Custkey')['Sales Amount'].sum()

# Step 2b: Total Purchases per Customer
customer_purchases = df.groupby('Custkey')['Sales Quantity'].sum()

# Step 2c: Average Purchase Value (APV)
average_purchase_value = customer_revenue.mean()

# Step 2d: Average Purchase Frequency (APF)
unique_customers = df['Custkey'].nunique()  # Total unique customers
total_purchases = df['Sales Quantity'].sum()  # Total purchases
average_purchase_frequency = total_purchases / unique_customers

# Step 2e: Customer Value (CV)
customer_value = average_purchase_value * average_purchase_frequency

# Step 2f: Customer Lifetime (CL)
# Assuming a churn rate of 20% as an example
churn_rate = 0.20
customer_lifetime = 1 / churn_rate

# Step 3: Calculate Customer Lifetime Value (CLV)
clv = customer_value * customer_lifetime

# Print results
print("Average Purchase Value (APV):", average_purchase_value)
print("Average Purchase Frequency (APF):", average_purchase_frequency)
print("Customer Value (CV):", customer_value)
print("Customer Lifetime (CL):", customer_lifetime)
print("Customer Lifetime Value (CLV):", clv)

# Import required libraries
import pandas as pd
from textblob import TextBlob
from google.colab import files

# Step 1: Upload the dataset
uploaded = files.upload()  # Upload the file
file_name = list(uploaded.keys())[0]
data = pd.ExcelFile(file_name)

# Step 2: Load the relevant sheet (update the sheet name accordingly)
df = data.parse('AmazonFoodCategory_1 Dataset')  # Change sheet name as needed

# Step 3: Select the text column for sentiment analysis
# Assume the column 'Item' contains text descriptions
text_column = 'Item'  # Update with the name of your text column

# Check if the column exists
if text_column in df.columns:
    # Step 4: Perform sentiment analysis
    def analyze_sentiment(text):
        blob = TextBlob(str(text))
        return blob.sentiment.polarity, blob.sentiment.subjectivity

    # Apply sentiment analysis to the text column
    df[['Polarity', 'Subjectivity']] = df[text_column].apply(analyze_sentiment).apply(pd.Series)

    # Step 5: Label the sentiment based on polarity
    def label_sentiment(polarity):
        if polarity > 0:
            return "Positive"
        elif polarity < 0:
            return "Negative"
        else:
            return "Neutral"

    df['Sentiment'] = df['Polarity'].apply(label_sentiment)

    # Display results
    print(df[['Item', 'Polarity', 'Subjectivity', 'Sentiment']].head())

    # Save results to a new file
    df.to_excel("Sentiment_Analysis_Results.xlsx", index=False)
    print("Sentiment analysis results saved as Sentiment_Analysis_Results.xlsx")
else:
    print(f"Column '{text_column}' not found in the dataset.")